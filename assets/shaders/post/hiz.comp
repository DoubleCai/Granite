#version 450
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_quad : require

// A rewrite of SPD to support HiZ correctly and moar wave ops for good measure.

layout(local_size_x = 256) in;

layout(set = 0, binding = 0, r32f) uniform coherent image2D uImages[13];
layout(set = 1, binding = 0) uniform sampler2D uTexture;
layout(set = 1, binding = 1) buffer Counter
{
    uint atomic_counter;
};

layout(push_constant, std430) uniform Registers
{
    mat2 z_transform;
    ivec2 resolution;
    vec2 inv_resolution;
    int mips;
    uint target_counter;
} registers;

ivec2 mip_resolution(int mip)
{
    return max(registers.resolution >> mip, ivec2(1));
}

#define REDUCE_OPERATOR max
float reduce(vec4 v)
{
    vec2 v0 = REDUCE_OPERATOR(v.xy, v.zw);
    return REDUCE_OPERATOR(v0.x, v0.y);
}

const int SHUFFLE_X0 = 1 << 0;
const int SHUFFLE_Y0 = 1 << 1;
const int SHUFFLE_Y1 = 1 << 2;
const int SHUFFLE_X1 = 1 << 3;

uvec2 unswizzle16x16(uint index)
{
    uint x0 = bitfieldExtract(index, 0, 1);
    uint y01 = bitfieldExtract(index, 1, 2);
    uint x12 = bitfieldExtract(index, 3, 2);
    uint y23 = bitfieldExtract(index, 5, 2);
    uint x3 = bitfieldExtract(index, 7, 1);
    return uvec2(bitfieldInsert(bitfieldInsert(x0, x12, 1, 2), x3, 3, 1), bitfieldInsert(y01, y23, 2, 2));
}

vec4 transform_z(vec4 zs)
{
    vec2 z0 = registers.z_transform * vec2(zs.x, 1.0);
    vec2 z1 = registers.z_transform * vec2(zs.y, 1.0);
    vec2 z2 = registers.z_transform * vec2(zs.z, 1.0);
    vec2 z3 = registers.z_transform * vec2(zs.w, 1.0);
    return vec4(z0.x, z1.x, z2.x, z3.x) / vec4(z0.y, z1.y, z2.y, z3.y);
}

void write_image(ivec2 coord, int mip, float v)
{
    // Rely on image robustness to clean up the OOB writes here.
    imageStore(uImages[mip], coord, vec4(v));
}

void write_image4(ivec2 coord, int mip, vec4 v)
{
    imageStore(uImages[mip], coord + ivec2(0, 0), v.xxxx);
    imageStore(uImages[mip], coord + ivec2(1, 0), v.yyyy);
    imageStore(uImages[mip], coord + ivec2(0, 1), v.zzzz);
    imageStore(uImages[mip], coord + ivec2(1, 1), v.wwww);
}

const int SHARED_WIDTH = 32;
const int SHARED_HEIGHT = 32;
const int BANK_STRIDE = SHARED_WIDTH * SHARED_HEIGHT;
shared float shared_buffer[2 * BANK_STRIDE];
shared bool shared_is_last_workgroup;

mat4 fetch_4x4_texture(ivec2 base_coord)
{
    vec2 fcoord = vec2(base_coord) * registers.inv_resolution;
    vec4 q00 = textureGatherOffset(uTexture, fcoord, ivec2(1, 1)).wzxy;
    vec4 q10 = textureGatherOffset(uTexture, fcoord, ivec2(3, 1)).wzxy;
    vec4 q01 = textureGatherOffset(uTexture, fcoord, ivec2(1, 3)).wzxy;
    vec4 q11 = textureGatherOffset(uTexture, fcoord, ivec2(3, 3)).wzxy;
    return mat4(q00, q10, q01, q11);
}

vec4 fetch_2x2_image_mip6(ivec2 base_coord)
{
    ivec2 max_coord = mip_resolution(6) - 1;
    float d0 = imageLoad(uImages[6], min(base_coord + ivec2(0, 0), max_coord)).x;
    float d1 = imageLoad(uImages[6], min(base_coord + ivec2(1, 0), max_coord)).x;
    float d2 = imageLoad(uImages[6], min(base_coord + ivec2(0, 1), max_coord)).x;
    float d3 = imageLoad(uImages[6], min(base_coord + ivec2(1, 1), max_coord)).x;
    return vec4(d0, d1, d2, d3);
}

float fetch_image_mip6(ivec2 coord)
{
    return imageLoad(uImages[6], coord).x;
}

mat4 write_mip0_transformed(mat4 M, ivec2 base_coord)
{
    vec4 q00 = transform_z(M[0]);
    vec4 q10 = transform_z(M[1]);
    vec4 q01 = transform_z(M[2]);
    vec4 q11 = transform_z(M[3]);

    // Write out transformed LOD 0
    write_image4(base_coord + ivec2(0, 0), 0, q00);
    write_image4(base_coord + ivec2(2, 0), 0, q10);
    write_image4(base_coord + ivec2(0, 2), 0, q01);
    write_image4(base_coord + ivec2(2, 2), 0, q11);

    return mat4(q00, q10, q01, q11);
}

// For LOD 0 to 6, it is expected that the division is exact,
// i.e., the lower resolution mip is exactly half resolution.
// This way we avoid needing to fold in neighbors.

float reduce_mip_registers(mat4 M, ivec2 base_coord, int mip)
{
    vec4 q00 = M[0];
    vec4 q10 = M[1];
    vec4 q01 = M[2];
    vec4 q11 = M[3];

    ivec2 mip_res = mip_resolution(mip);

    float d00 = reduce(q00);
    float d10 = reduce(q10);
    float d01 = reduce(q01);
    float d11 = reduce(q11);

    q00 = vec4(d00, d10, d01, d11);
    write_image4(base_coord, mip, q00);

    return reduce(q00);
}

void reduce_mip_shared(ivec2 base_coord, int mip)
{
    ivec2 mip_res_higher = mip_resolution(mip - 1);
    ivec2 mip_res_target = mip_resolution(mip);

    bool horiz_fold = base_coord.x + 1 == mip_res_target.x && (mip_res_higher.x & 1) != 0;
    bool vert_fold = base_coord.y + 1 == mip_res_target.y && (mip_res_higher.y & 1) != 0;
    bool diag_fold = horiz_fold && vert_fold;

    const int DOUBLE_SHARED_WIDTH = SHARED_WIDTH * 2;

    // Ping-pong the shared buffer to avoid double barrier.
    int out_offset = (mip & 1) * BANK_STRIDE;
    int in_offset = BANK_STRIDE - out_offset;
    int base_in_coord = in_offset + base_coord.y * DOUBLE_SHARED_WIDTH + base_coord.x * 2;

    float d00 = shared_buffer[base_in_coord];
    float d10 = shared_buffer[base_in_coord + 1];
    float d01 = shared_buffer[base_in_coord + SHARED_WIDTH];
    float d11 = shared_buffer[base_in_coord + SHARED_WIDTH + 1];

    float reduced = reduce(vec4(d00, d10, d01, d11));

    if (horiz_fold)
    {
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[base_in_coord + 2]);
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[base_in_coord + 2 + SHARED_WIDTH]);
    }

    if (vert_fold)
    {
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[base_in_coord + DOUBLE_SHARED_WIDTH]);
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[base_in_coord + DOUBLE_SHARED_WIDTH + 1]);
    }

    if (diag_fold)
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[base_in_coord + DOUBLE_SHARED_WIDTH + 2]);

    shared_buffer[out_offset + base_coord.y * SHARED_WIDTH + base_coord.x] = reduced;
    write_image(base_coord, mip, reduced);
}

void reduce_mip_lod7(ivec2 base_coord)
{
    ivec2 mip_res6 = mip_resolution(6);
    ivec2 mip_res7 = mip_resolution(7);

    float reduced = reduce(fetch_2x2_image_mip6(2 * base_coord));

    // NPOT folding for LOD 7. Our group will write the edge,
    // so need to fold in any last neighbor in previous LOD which may contribute,
    // but would be otherwise lost to the rounding down.

    bool horiz_fold = base_coord.x + 1 == mip_res7.x && (mip_res6.x & 1) != 0;
    bool vert_fold = base_coord.y + 1 == mip_res7.y && (mip_res6.y & 1) != 0;
    bool diag_fold = horiz_fold && vert_fold;

    if (horiz_fold)
    {
        reduced = REDUCE_OPERATOR(reduced, fetch_image_mip6(2 * base_coord + ivec2(2, 0)));
        reduced = REDUCE_OPERATOR(reduced, fetch_image_mip6(2 * base_coord + ivec2(2, 1)));
    }

    if (vert_fold)
    {
        reduced = REDUCE_OPERATOR(reduced, fetch_image_mip6(2 * base_coord + ivec2(0, 2)));
        reduced = REDUCE_OPERATOR(reduced, fetch_image_mip6(2 * base_coord + ivec2(1, 2)));
    }

    if (diag_fold)
        reduced = REDUCE_OPERATOR(reduced, fetch_image_mip6(2 * base_coord + ivec2(2, 2)));

    write_image(base_coord, 7, reduced);
    shared_buffer[BANK_STRIDE + base_coord.y * SHARED_WIDTH + base_coord.x] = reduced;
}

float reduce_mips_simd16(ivec2 base_coord, uint local_index, int mip, float d)
{
    ivec2 mip_res = mip_resolution(mip);
    float d_horiz, d_vert, d_diag;
    bool swap_horiz, swap_vert;

    d_horiz = subgroupQuadSwapHorizontal(d);
    d_vert = subgroupQuadSwapVertical(d);
    d_diag = subgroupQuadSwapDiagonal(d);
    write_image(base_coord, mip, d);

    if (registers.mips > mip + 1)
    {
        base_coord >>= 1;
        mip_res = mip_resolution(mip + 1);
        d = reduce(vec4(d, d_horiz, d_vert, d_diag));

        // This requires only SIMD16, which everyone can do.
        d_horiz = subgroupShuffleXor(d, SHUFFLE_X1);
        d_vert = subgroupShuffleXor(d, SHUFFLE_Y1);
        d_diag = subgroupShuffleXor(d, SHUFFLE_X1 | SHUFFLE_Y1);
        if ((local_index & 3) == 0)
            write_image(base_coord, mip + 1, d);
    }

    return reduce(vec4(d, d_horiz, d_vert, d_diag));
}

// Each workgroup reduces 64x64 on its own.
// Allows reducing up to a 4096x4096 texture, like SPD.

void main()
{
    uint local_index = gl_SubgroupID * gl_SubgroupSize + gl_SubgroupInvocationID;
    uvec2 local_coord = unswizzle16x16(local_index);

    // LOD 0 feedback
    ivec2 base_coord = ivec2(local_coord) * 4 + ivec2(gl_WorkGroupID.xy * 64u);
    mat4 M = fetch_4x4_texture(base_coord);
    M = write_mip0_transformed(M, base_coord);

    // Write LOD 1, Compute LOD 2
    if (registers.mips <= 1)
        return;
    float d = reduce_mip_registers(M, base_coord >> 1, 1);
    if (registers.mips <= 2)
        return;

    // Write LOD 2, Compute LOD 3-4
    d = reduce_mips_simd16(base_coord >> 2, local_index, 2, d);
    if (registers.mips <= 4)
        return;

    // Write LOD 4 to shared
    if ((local_index & 15) == 0)
        shared_buffer[local_index >> 4] = d;
    barrier();

    // Write LOD 4, Compute LOD 5-6.
    if (local_index < 16)
        d = reduce_mips_simd16(ivec2(gl_WorkGroupID.xy * 4u + local_coord), local_index, 4, shared_buffer[local_index]);

    // Write LOD 6.
    if (registers.mips <= 6)
        return;
    if (local_index == 0)
        write_image(ivec2(gl_WorkGroupID.xy), 6, d);
    if (registers.mips <= 7)
        return;

    // Persistent waves
    memoryBarrierImage();
    if (local_index == 0)
        shared_is_last_workgroup = atomicAdd(atomic_counter, 1u) + 1u == registers.target_counter;
    barrier();
    if (!shared_is_last_workgroup)
        return;

    // Reset counter for next iteration.
    if (local_index == 0)
        atomic_counter = 0u;

    // At this point, the mip resolutions may be non-POT and things get spicy.
    // Not using subgroup ops anymore, so use straight linear coordinates.
    local_coord.x = bitfieldExtract(local_index, 0, 4);
    local_coord.y = bitfieldExtract(local_index, 4, 4);

    // Write LOD 7-8, Compute LOD 8
    ivec2 mip_res7 = mip_resolution(7);
    for (int y = 0; y < mip_res7.y; y += 16)
        for (int x = 0; x < mip_res7.x; x += 16)
            reduce_mip_lod7(ivec2(local_coord) + ivec2(x, y));

    if (registers.mips <= 8)
        return;
    barrier();
    reduce_mip_shared(ivec2(local_coord), 8);

    if (registers.mips <= 9)
        return;
    barrier();
    if (local_index < 64)
        reduce_mip_shared(ivec2(local_coord), 9);

    if (registers.mips <= 10)
        return;
    barrier();
    if (local_index < 16)
        reduce_mip_shared(ivec2(local_coord), 10);

    if (registers.mips <= 11)
        return;
    barrier();
    if (local_index < 4)
        reduce_mip_shared(ivec2(local_coord), 11);

    if (registers.mips <= 12)
        return;
    barrier();
    if (local_index == 0)
        reduce_mip_shared(ivec2(0), 12);
}
