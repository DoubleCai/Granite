#version 450
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_quad : require

// A rewrite of SPD to support HiZ correctly and moar wave ops for good measure.

layout(local_size_x = 256) in;

layout(set = 0, binding = 0, r32f) uniform coherent image2D uImages[13];
layout(set = 1, binding = 0) uniform sampler2D uTexture;
layout(set = 1, binding = 1) buffer Counter
{
    uint atomic_counter;
};

layout(push_constant, std430) uniform Registers
{
    mat2 z_transform;
    ivec2 resolution;
    vec2 inv_resolution;
    int mips;
    uint target_counter;
} registers;

ivec2 mip_resolution(int mip)
{
    return max(registers.resolution >> mip, ivec2(1));
}

#define REDUCE_OPERATOR max
float reduce(vec4 v)
{
    vec2 v0 = REDUCE_OPERATOR(v.xy, v.zw);
    return REDUCE_OPERATOR(v0.x, v0.y);
}

const int SHUFFLE_X0 = 1 << 0;
const int SHUFFLE_Y0 = 1 << 1;
const int SHUFFLE_Y1 = 1 << 2;
const int SHUFFLE_X1 = 1 << 3;

uvec2 unswizzle16x16(uint index)
{
    uint x0 = bitfieldExtract(index, 0, 1);
    uint y01 = bitfieldExtract(index, 1, 2);
    uint x12 = bitfieldExtract(index, 3, 2);
    uint y23 = bitfieldExtract(index, 5, 2);
    uint x3 = bitfieldExtract(index, 7, 1);
    return uvec2(bitfieldInsert(bitfieldInsert(x0, x12, 1, 2), x3, 3, 1), bitfieldInsert(y01, y23, 2, 2));
}

vec4 transform_z(vec4 zs)
{
    vec2 z0 = registers.z_transform * vec2(zs.x, 1.0);
    vec2 z1 = registers.z_transform * vec2(zs.y, 1.0);
    vec2 z2 = registers.z_transform * vec2(zs.z, 1.0);
    vec2 z3 = registers.z_transform * vec2(zs.w, 1.0);
    return vec4(z0.x, z1.x, z2.x, z3.x) / vec4(z0.y, z1.y, z2.y, z3.y);
}

void write_image(ivec2 coord, int mip, float v)
{
    // Rely on image robustness to clean up the OOB writes here.
    imageStore(uImages[mip], coord, vec4(v));
}

void write_image4(ivec2 coord, int mip, vec4 v)
{
    imageStore(uImages[mip], coord + ivec2(0, 0), v.xxxx);
    imageStore(uImages[mip], coord + ivec2(1, 0), v.yyyy);
    imageStore(uImages[mip], coord + ivec2(0, 1), v.zzzz);
    imageStore(uImages[mip], coord + ivec2(1, 1), v.wwww);
}

shared float shared_buffer[128];
shared bool shared_is_last_workgroup;

mat4 fetch_4x4_texture(ivec2 base_coord)
{
    vec2 fcoord = vec2(base_coord) * registers.inv_resolution;
    vec4 q00 = textureGatherOffset(uTexture, fcoord, ivec2(1, 1)).wzxy;
    vec4 q10 = textureGatherOffset(uTexture, fcoord, ivec2(3, 1)).wzxy;
    vec4 q01 = textureGatherOffset(uTexture, fcoord, ivec2(1, 3)).wzxy;
    vec4 q11 = textureGatherOffset(uTexture, fcoord, ivec2(3, 3)).wzxy;
    return mat4(q00, q10, q01, q11);
}

vec4 fetch_2x2_image_mip6(ivec2 base_coord)
{
    ivec2 max_coord = mip_resolution(6) - 1;
    float d0 = imageLoad(uImages[6], min(base_coord + ivec2(0, 0), max_coord)).x;
    float d1 = imageLoad(uImages[6], min(base_coord + ivec2(1, 0), max_coord)).x;
    float d2 = imageLoad(uImages[6], min(base_coord + ivec2(0, 1), max_coord)).x;
    float d3 = imageLoad(uImages[6], min(base_coord + ivec2(1, 1), max_coord)).x;
    return vec4(d0, d1, d2, d3);
}

float fetch_image_mip6(ivec2 coord)
{
    ivec2 max_coord = mip_resolution(6) - 1;
    return imageLoad(uImages[6], min(coord, max_coord)).x;
}

mat4 write_mip0_transformed(mat4 M, ivec2 base_coord)
{
    vec4 q00 = transform_z(M[0]);
    vec4 q10 = transform_z(M[1]);
    vec4 q01 = transform_z(M[2]);
    vec4 q11 = transform_z(M[3]);

    // Write out transformed LOD 0
    write_image4(base_coord + ivec2(0, 0), 0, q00);
    write_image4(base_coord + ivec2(2, 0), 0, q10);
    write_image4(base_coord + ivec2(0, 2), 0, q01);
    write_image4(base_coord + ivec2(2, 2), 0, q11);

    return mat4(q00, q10, q01, q11);
}

// For LOD 0 to 6, it is expected that the division is exact,
// i.e., the lower resolution mip is exactly half resolution.
// This way we avoid needing to fold in neighbors.

float reduce_mip_registers(mat4 M, ivec2 base_coord, int mip)
{
    vec4 q00 = M[0];
    vec4 q10 = M[1];
    vec4 q01 = M[2];
    vec4 q11 = M[3];

    ivec2 mip_res = mip_resolution(mip);

    float d00 = reduce(q00);
    float d10 = reduce(q10);
    float d01 = reduce(q01);
    float d11 = reduce(q11);

    q00 = vec4(d00, d10, d01, d11);
    write_image4(base_coord, mip, q00);

    return reduce(q00);
}

void reduce_mip_shared(ivec2 base_coord, int mip)
{
    ivec2 mip_res_higher = mip_resolution(mip - 1);
    ivec2 mip_res_target = mip_resolution(mip);

    bool horiz_fold = base_coord.x + 1 == mip_res_target.x && (mip_res_higher.x & 1) != 0;
    bool vert_fold = base_coord.y + 1 == mip_res_target.y && (mip_res_higher.y & 1) != 0;
    bool diag_fold = horiz_fold && vert_fold;

    // Ping-pong the shared buffer to avoid double barrier.
    uint out_offset = (mip & 1) * 64;
    uint in_offset = 64 - out_offset;

    float d00 = shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2];
    float d10 = shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 1];
    float d01 = shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 16];
    float d11 = shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 17];

    float reduced = reduce(vec4(d00, d10, d01, d11));
    if (horiz_fold)
    {
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 2]);
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 18]);
    }

    if (vert_fold)
    {
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 32]);
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 33]);
    }

    if (diag_fold)
        reduced = REDUCE_OPERATOR(reduced, shared_buffer[in_offset + base_coord.y * 32 + base_coord.x * 2 + 34]);

    shared_buffer[out_offset + base_coord.y * 16 + base_coord.x] = reduced;
    write_image(base_coord, mip, reduced);
}

void reduce_mip_lod7_lod8(ivec2 base_coord_lod8)
{
    ivec2 mip_res6 = mip_resolution(6);
    ivec2 mip_res7 = mip_resolution(7);
    ivec2 base_coord_lod6 = base_coord_lod8 * 4;
    ivec2 base_coord_lod7 = base_coord_lod8 * 2;

    float d00 = reduce(fetch_2x2_image_mip6(base_coord_lod6 + ivec2(0, 0)));
    float d10 = reduce(fetch_2x2_image_mip6(base_coord_lod6 + ivec2(2, 0)));
    float d01 = reduce(fetch_2x2_image_mip6(base_coord_lod6 + ivec2(0, 2)));
    float d11 = reduce(fetch_2x2_image_mip6(base_coord_lod6 + ivec2(2, 2)));

    // NPOT folding for LOD 7. Our group will write the edge,
    // so need to fold in any last neighbor in previous LOD which may contribute,
    // but would be otherwise lost to the rounding down.

    bool horiz_fold = base_coord_lod7.x + 2 == mip_res7.x && (mip_res6.x & 1) != 0;
    bool vert_fold = base_coord_lod7.y + 2 == mip_res7.y && (mip_res6.y & 1) != 0;
    bool diag_fold = horiz_fold && vert_fold;

    if (horiz_fold)
    {
        float d20 = REDUCE_OPERATOR(
            fetch_image_mip6(base_coord_lod6 + ivec2(4, 0)),
            fetch_image_mip6(base_coord_lod6 + ivec2(4, 1)));

        float d21 = REDUCE_OPERATOR(
            fetch_image_mip6(base_coord_lod6 + ivec2(4, 2)),
            fetch_image_mip6(base_coord_lod6 + ivec2(4, 3)));

        d10 = REDUCE_OPERATOR(d10, d20);
        d11 = REDUCE_OPERATOR(d11, d21);
    }

    if (vert_fold)
    {
        float d02 = REDUCE_OPERATOR(
            fetch_image_mip6(base_coord_lod6 + ivec2(0, 4)),
            fetch_image_mip6(base_coord_lod6 + ivec2(1, 4)));

        float d12 = REDUCE_OPERATOR(
            fetch_image_mip6(base_coord_lod6 + ivec2(2, 4)),
            fetch_image_mip6(base_coord_lod6 + ivec2(3, 4)));

        d01 = REDUCE_OPERATOR(d01, d02);
        d11 = REDUCE_OPERATOR(d11, d12);
    }

    if (diag_fold)
    {
        float d22 = fetch_image_mip6(base_coord_lod6 + ivec2(4, 4));
        d11 = REDUCE_OPERATOR(d11, d22);
    }

    // If the edge pixels will be dropped, fold them into the top-left pixel.
    horiz_fold = base_coord_lod7.x + 1 == mip_res7.x;
    vert_fold = base_coord_lod7.y + 1 == mip_res7.y;
    diag_fold = horiz_fold && vert_fold;

    if (horiz_fold)
    {
        d00 = REDUCE_OPERATOR(d00, d10);
        d01 = REDUCE_OPERATOR(d01, d11);
    }

    if (vert_fold)
    {
        d00 = REDUCE_OPERATOR(d00, d01);
        d10 = REDUCE_OPERATOR(d10, d11);
    }

    if (diag_fold)
        d00 = REDUCE_OPERATOR(d00, d11);

    vec4 quad = vec4(d00, d10, d01, d11);
    write_image4(base_coord_lod7, 7, quad);

    if (registers.mips > 8)
    {
        float lod8 = reduce(quad);
        shared_buffer[base_coord_lod8.y * 16 + base_coord_lod8.x] = lod8;

        // If writes to mip 8 may be sliced, fixup.
        if (((mip_res7.x | mip_res7.y) & 1) != 0)
        {
            barrier();

            ivec2 mip_res8 = mip_resolution(8);
            horiz_fold = base_coord_lod8.x + 1 == mip_res8.x && (mip_res7.x & 1) != 0;
            vert_fold = base_coord_lod8.y + 1 == mip_res8.y && (mip_res7.y & 1) != 0;
            diag_fold = horiz_fold && vert_fold;

            if (horiz_fold)
                lod8 = REDUCE_OPERATOR(lod8, shared_buffer[base_coord_lod8.y * 16 + base_coord_lod8.x + 1]);
            if (vert_fold)
                lod8 = REDUCE_OPERATOR(lod8, shared_buffer[base_coord_lod8.y * 16 + base_coord_lod8.x + 16]);
            if (diag_fold)
                lod8 = REDUCE_OPERATOR(lod8, shared_buffer[base_coord_lod8.y * 16 + base_coord_lod8.x + 17]);
            if (horiz_fold || vert_fold)
                shared_buffer[base_coord_lod8.y * 16 + base_coord_lod8.x] = lod8;
        }

        write_image(base_coord_lod8, 8, lod8);
    }
}

float reduce_mips_simd16(ivec2 base_coord, uint local_index, int mip, float d)
{
    ivec2 mip_res = mip_resolution(mip);
    float d_horiz, d_vert, d_diag;
    bool swap_horiz, swap_vert;

    d_horiz = subgroupQuadSwapHorizontal(d);
    d_vert = subgroupQuadSwapVertical(d);
    d_diag = subgroupQuadSwapDiagonal(d);
    write_image(base_coord, mip, d);

    if (registers.mips > mip + 1)
    {
        base_coord >>= 1;
        mip_res = mip_resolution(mip + 1);
        d = reduce(vec4(d, d_horiz, d_vert, d_diag));

        // This requires only SIMD16, which everyone can do.
        d_horiz = subgroupShuffleXor(d, SHUFFLE_X1);
        d_vert = subgroupShuffleXor(d, SHUFFLE_Y1);
        d_diag = subgroupShuffleXor(d, SHUFFLE_X1 | SHUFFLE_Y1);
        if ((local_index & 3) == 0)
            write_image(base_coord, mip + 1, d);
    }

    return reduce(vec4(d, d_horiz, d_vert, d_diag));
}

// Each workgroup reduces 64x64 on its own.
// Allows reducing up to a 4096x4096 texture, like SPD.

void main()
{
    uint local_index = gl_SubgroupID * gl_SubgroupSize + gl_SubgroupInvocationID;
    uvec2 local_coord = unswizzle16x16(local_index);

    // LOD 0 feedback
    ivec2 base_coord = ivec2(local_coord) * 4 + ivec2(gl_WorkGroupID.xy * 64u);
    mat4 M = fetch_4x4_texture(base_coord);
    M = write_mip0_transformed(M, base_coord);

    // Write LOD 1, Compute LOD 2
    if (registers.mips <= 1)
        return;
    float d = reduce_mip_registers(M, base_coord >> 1, 1);
    if (registers.mips <= 2)
        return;

    // Write LOD 2, Compute LOD 3-4
    d = reduce_mips_simd16(base_coord >> 2, local_index, 2, d);
    if (registers.mips <= 4)
        return;

    // Write LOD 4 to shared
    if ((local_index & 15) == 0)
        shared_buffer[local_index >> 4] = d;
    barrier();

    // Write LOD 4, Compute LOD 5-6.
    if (local_index < 16)
        d = reduce_mips_simd16(ivec2(gl_WorkGroupID.xy * 4u + local_coord), local_index, 4, shared_buffer[local_index]);

    // Write LOD 6.
    if (registers.mips <= 6)
        return;
    if (local_index == 0)
        write_image(ivec2(gl_WorkGroupID.xy), 6, d);
    if (registers.mips <= 7)
        return;

    // Persistent waves
    memoryBarrierImage();
    if (local_index == 0)
        shared_is_last_workgroup = atomicAdd(atomic_counter, 1u) + 1u == registers.target_counter;
    barrier();
    if (!shared_is_last_workgroup)
        return;

    // Reset counter for next iteration.
    if (local_index == 0)
        atomic_counter = 0u;

    // At this point, the mip resolutions may be non-POT and things get spicy.

    // Write LOD 7-8, Compute LOD 8
    reduce_mip_lod7_lod8(ivec2(local_coord));

    if (registers.mips <= 9)
        return;
    barrier();
    if (local_index < 64)
        reduce_mip_shared(ivec2(local_coord), 9);

    if (registers.mips <= 10)
        return;
    barrier();
    if (local_index < 16)
        reduce_mip_shared(ivec2(local_coord), 10);

    if (registers.mips <= 11)
        return;
    barrier();
    if (local_index < 4)
        reduce_mip_shared(ivec2(local_coord), 11);

    if (registers.mips <= 12)
        return;
    barrier();
    if (local_index == 0)
        reduce_mip_shared(ivec2(0), 12);
}
