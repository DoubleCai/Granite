#version 450
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_quad : require

// A rewrite of SPD to support HiZ correctly and moar wave ops for good measure.

layout(local_size_x = 256) in;

layout(set = 0, binding = 0, r32f) uniform coherent image2D uImages[13];
layout(set = 1, binding = 0) uniform sampler2D uTexture;
layout(set = 1, binding = 1) buffer Counter
{
    uint atomic_counter;
};

layout(push_constant, std430) uniform Registers
{
    mat2 z_transform;
    ivec2 resolution;
    vec2 inv_resolution;
    int mips;
    uint target_counter;
} registers;

ivec2 mip_resolution(int mip)
{
    return max(registers.resolution >> mip, ivec2(1));
}

#define REDUCE_OPERATOR max
float reduce(vec4 v)
{
    vec2 v0 = REDUCE_OPERATOR(v.xy, v.zw);
    return REDUCE_OPERATOR(v0.x, v0.y);
}

const int SHUFFLE_X0 = 1 << 0;
const int SHUFFLE_Y0 = 1 << 1;
const int SHUFFLE_Y1 = 1 << 2;
const int SHUFFLE_X1 = 1 << 3;

uvec2 unswizzle16x16(uint index)
{
    uint x0 = bitfieldExtract(index, 0, 1);
    uint y01 = bitfieldExtract(index, 1, 2);
    uint x12 = bitfieldExtract(index, 3, 2);
    uint y23 = bitfieldExtract(index, 5, 2);
    uint x3 = bitfieldExtract(index, 7, 1);
    return uvec2(bitfieldInsert(bitfieldInsert(x0, x12, 1, 2), x3, 3, 1), bitfieldInsert(y01, y23, 2, 2));
}

vec4 transform_z(vec4 zs)
{
    vec2 z0 = registers.z_transform * vec2(zs.x, 1.0);
    vec2 z1 = registers.z_transform * vec2(zs.y, 1.0);
    vec2 z2 = registers.z_transform * vec2(zs.z, 1.0);
    vec2 z3 = registers.z_transform * vec2(zs.w, 1.0);
    return vec4(z0.x, z1.x, z2.x, z3.x) / vec4(z0.y, z1.y, z2.y, z3.y);
}

void write_image(ivec2 coord, int mip, float v)
{
    // Rely on image robustness to clean up the OOB writes here.
    imageStore(uImages[mip], coord, vec4(v));
}

void write_image4(ivec2 coord, int mip, vec4 v)
{
    imageStore(uImages[mip], coord + ivec2(0, 0), v.xxxx);
    imageStore(uImages[mip], coord + ivec2(1, 0), v.yyyy);
    imageStore(uImages[mip], coord + ivec2(0, 1), v.zzzz);
    imageStore(uImages[mip], coord + ivec2(1, 1), v.wwww);
}

shared float shared_buffer[256 / 16];
shared bool shared_is_last_workgroup;

mat4 fetch_4x4_texture(ivec2 base_coord)
{
    vec2 fcoord = vec2(base_coord) * registers.inv_resolution;
    vec4 q00 = textureGatherOffset(uTexture, fcoord, ivec2(1, 1)).wzxy;
    vec4 q10 = textureGatherOffset(uTexture, fcoord, ivec2(3, 1)).wzxy;
    vec4 q01 = textureGatherOffset(uTexture, fcoord, ivec2(1, 3)).wzxy;
    vec4 q11 = textureGatherOffset(uTexture, fcoord, ivec2(3, 3)).wzxy;
    return mat4(q00, q10, q01, q11);
}

vec4 fetch_2x2_image_mip6(ivec2 base_coord)
{
    ivec2 max_coord = mip_resolution(6) - 1;
    float d0 = imageLoad(uImages[6], min(base_coord + ivec2(0, 0), max_coord)).x;
    float d1 = imageLoad(uImages[6], min(base_coord + ivec2(1, 0), max_coord)).x;
    float d2 = imageLoad(uImages[6], min(base_coord + ivec2(0, 1), max_coord)).x;
    float d3 = imageLoad(uImages[6], min(base_coord + ivec2(1, 1), max_coord)).x;
    return vec4(d0, d1, d2, d3);
}

mat4 fetch_4x4_image_mip6(ivec2 base_coord)
{
    vec4 q0 = fetch_2x2_image_mip6(base_coord + ivec2(0, 0));
    vec4 q1 = fetch_2x2_image_mip6(base_coord + ivec2(2, 0));
    vec4 q2 = fetch_2x2_image_mip6(base_coord + ivec2(0, 2));
    vec4 q3 = fetch_2x2_image_mip6(base_coord + ivec2(2, 2));
    return mat4(q0, q1, q2, q3);
}

mat4 write_mip0_transformed(mat4 M, ivec2 base_coord)
{
    vec4 q00 = transform_z(M[0]);
    vec4 q10 = transform_z(M[1]);
    vec4 q01 = transform_z(M[2]);
    vec4 q11 = transform_z(M[3]);

    // Write out transformed LOD 0
    write_image4(base_coord + ivec2(0, 0), 0, q00);
    write_image4(base_coord + ivec2(2, 0), 0, q10);
    write_image4(base_coord + ivec2(0, 2), 0, q01);
    write_image4(base_coord + ivec2(2, 2), 0, q11);

    return mat4(q00, q10, q01, q11);
}

float reduce_mip_registers(mat4 M, ivec2 base_coord, int mip, bool full_res_pass)
{
    vec4 q00 = M[0];
    vec4 q10 = M[1];
    vec4 q01 = M[2];
    vec4 q11 = M[3];

    ivec2 mip_res = mip_resolution(mip);

    float d00 = reduce(q00);
    float d10 = reduce(q10);
    float d01 = reduce(q01);
    float d11 = reduce(q11);

    if (!full_res_pass)
    {
        if (base_coord.x + 1 == mip_res.x) // LOD math chops off data. Need to fold border values into the reduction.
        {
            d00 = REDUCE_OPERATOR(d00, d10);
            d01 = REDUCE_OPERATOR(d01, d11);
        }

        if (base_coord.y + 1 == mip_res.y)
        {
            d01 = REDUCE_OPERATOR(d01, d00);
            d11 = REDUCE_OPERATOR(d11, d10);
        }
    }

    q00 = vec4(d00, d10, d01, d11);
    write_image4(base_coord, mip, q00);

    return reduce(q00);
}

float reduce_mips_simd16(ivec2 base_coord, uint local_index, int mip, float d, bool full_res_pass)
{
    ivec2 mip_res = mip_resolution(mip);
    float d_horiz, d_vert, d_diag;
    bool swap_horiz, swap_vert;

    // It is possible that our thread is barely in range, but horiz/vert neighbor is not.
#define CUTOFF_REDUCE() { \
    swap_horiz = base_coord.x + 1 == mip_res.x; \
    swap_vert = base_coord.y + 1 == mip_res.y; \
    if (swap_horiz) \
        d = REDUCE_OPERATOR(d, d_horiz); \
    if (swap_vert) \
        d = REDUCE_OPERATOR(d, d_vert); \
    if (swap_vert && swap_horiz) \
        d = REDUCE_OPERATOR(d, d_diag); }

    d_horiz = subgroupQuadSwapHorizontal(d);
    d_vert = subgroupQuadSwapVertical(d);
    d_diag = subgroupQuadSwapDiagonal(d);
    if (!full_res_pass)
        CUTOFF_REDUCE();
    write_image(base_coord, mip, d);

    if (registers.mips > mip + 1)
    {
        base_coord >>= 1;
        mip_res = mip_resolution(mip + 1);
        d = reduce(vec4(d, d_horiz, d_vert, d_diag));

        // This requires only SIMD16, which everyone can do.
        d_horiz = subgroupShuffleXor(d, SHUFFLE_X1);
        d_vert = subgroupShuffleXor(d, SHUFFLE_Y1);
        d_diag = subgroupShuffleXor(d, SHUFFLE_X1 | SHUFFLE_Y1);
        if (!full_res_pass)
            CUTOFF_REDUCE();
        if ((local_index & 3) == 0)
            write_image(base_coord, mip + 1, d);
    }

    return reduce(vec4(d, d_horiz, d_vert, d_diag));
}

// Each workgroup reduces 64x64 on its own.
// Allows reducing up to a 4096x4096 texture, like SPD.

void main()
{
    uint local_index = gl_SubgroupID * gl_SubgroupSize + gl_SubgroupInvocationID;
    uvec2 local_coord = unswizzle16x16(local_index);

    // LOD 0 feedback
    ivec2 base_coord = ivec2(local_coord) * 4 + ivec2(gl_WorkGroupID.xy * 64u);
    mat4 M = fetch_4x4_texture(base_coord);
    M = write_mip0_transformed(M, base_coord);

    // Write LOD 1, Compute LOD 2
    if (registers.mips <= 1)
        return;
    float d = reduce_mip_registers(M, base_coord >> 1, 1, true);
    if (registers.mips <= 2)
        return;

    // Write LOD 2, Compute LOD 3-4
    d = reduce_mips_simd16(base_coord >> 2, local_index, 2, d, true);
    if (registers.mips <= 4)
        return;

    // Write LOD 4 to shared
    if ((local_index & 15) == 0)
        shared_buffer[local_index >> 4] = d;
    barrier();

    // Write LOD 4, Compute LOD 5-6.
    if (local_index < 16)
        d = reduce_mips_simd16(ivec2(gl_WorkGroupID.xy * 4u + local_coord), local_index, 4, shared_buffer[local_index], true);

    // Write LOD 6.
    if (registers.mips <= 6)
        return;
    if (local_index == 0)
        write_image(ivec2(gl_WorkGroupID.xy), 6, d);
    if (registers.mips <= 7)
        return;

    // Persistent waves
    memoryBarrierImage();
    if (local_index == 0)
        shared_is_last_workgroup = atomicAdd(atomic_counter, 1u) + 1u == registers.target_counter;
    barrier();
    if (!shared_is_last_workgroup)
        return;

    // Reset counter for next iteration.
    if (local_index == 0)
        atomic_counter = 0u;

    // Write LOD 7, Compute LOD 8
    base_coord = ivec2(local_coord) * 4;
    d = reduce_mip_registers(fetch_4x4_image_mip6(base_coord), base_coord >> 1, 7, false);
    if (registers.mips <= 8)
        return;

    // Write LOD 8-9, Compute LOD 10
    d = reduce_mips_simd16(ivec2(local_coord), local_index, 8, d, false);
    if (registers.mips <= 10)
        return;
    if ((local_index & 15) == 0)
        shared_buffer[local_index >> 4] = d;
    barrier();

    if (local_index < 16)
        d = reduce_mips_simd16(ivec2(local_coord), local_index, 10, shared_buffer[local_index], false);
    if (registers.mips <= 12)
        return;
    if (local_index == 0)
        write_image(ivec2(0), 12, d);
}
